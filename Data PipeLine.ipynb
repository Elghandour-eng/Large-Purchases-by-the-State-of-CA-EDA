{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline and Data Engineering Best Practices\n",
    "\n",
    "This notebook serves as the next step following our data exploration and the completion of the Exploratory Data Analysis (EDA) process, as detailed in the [Data Exploration Notebook](https://github.com/Elghandour-eng/Large-Purchases-by-the-State-of-CA-EDA/blob/main/Data-Exploration.ipynb).\n",
    "\n",
    "## Objective\n",
    "\n",
    "The primary goal of this notebook is to implement a data pipeline for cleaning and reformatting the data structure. This will ensure that the data is more manageable and does not affect our original dataset. By storing the processed data in a separate collection in MongoDB, we create a \"data lake\" that allows us to perform operations on a different dataset. This approach adheres to data engineering best practices by ensuring separation and maintaining the integrity of the original data.\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. **Data Cleaning**: Remove any inconsistencies or errors in the data to ensure accuracy.\n",
    "2. **Data Reformatting**: Adjust the data structure to enhance usability and accessibility.\n",
    "3. **Data Storage**: Store the cleaned and reformatted data in a separate MongoDB collection to preserve the original dataset.\n",
    "4. **Data Lake Creation**: Establish a data lake to facilitate operations on the processed data without impacting the base data.\n",
    "5. **Best Practices**: Follow data engineering best practices to ensure efficient and effective data management.\n",
    "\n",
    "By following these steps, we aim to optimize our data handling processes and support robust data analysis and engineering efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- FirstStage\n",
    "\n",
    "This module defines a framework for creating and executing data pipelines.\n",
    "\n",
    "### Classes\n",
    "\n",
    "#### `Step`\n",
    "\n",
    "- **Description**: An abstract class representing a step in a data pipeline.\n",
    "- **Method**: \n",
    "  - `run(data)`: Abstract method to be implemented by subclasses, defining the operation on the input data.\n",
    "\n",
    "#### `Pipeline`\n",
    "\n",
    "- **Description**: Represents a data pipeline composed of multiple steps.\n",
    "- **Methods**:\n",
    "  - `__init__(steps)`: Initializes the pipeline with a list of `Step` instances.\n",
    "  - `execute(data)`: Runs each step in sequence on the input data, returning the final processed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Don't print warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Step(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class representing a step in a data pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, data):\n",
    "        \"\"\"\n",
    "        Abstract method to be implemented by subclasses.\n",
    "        This method will define the operation to be performed on the data.\n",
    "        \n",
    "        :param data: The input data to be processed.\n",
    "        :return: The processed data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Class representing a data pipeline consisting of multiple steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, steps):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with a list of steps.\n",
    "\n",
    "        :param steps: A list of Step instances to be executed in sequence.\n",
    "        \"\"\"\n",
    "        self.steps = steps\n",
    "\n",
    "    def execute(self, data):\n",
    "        \"\"\"\n",
    "        Execute the pipeline by running each step in sequence.\n",
    "\n",
    "        :param data: The initial input data to be processed through the pipeline.\n",
    "        :return: The final processed data after all steps have been executed.\n",
    "        \"\"\"\n",
    "        logging.info(\"Starting pipeline execution\")\n",
    "        for step in self.steps:\n",
    "            try:\n",
    "                logging.info(f\"Executing step: {step.__class__.__name__}\")\n",
    "                data = step.run(data)\n",
    "                logging.info(f\"Completed step: {step.__class__.__name__}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in step {step.__class__.__name__}: {e}\")\n",
    "                raise\n",
    "        logging.info(\"Pipeline execution completed\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- SecondStage\n",
    "\n",
    "The `SecondStage` is a critical phase in the data management process, utilizing the `MongoManagement` class to streamline operations involving MongoDB. This stage encompasses the following key activities:\n",
    "\n",
    "- **Data Export**: \n",
    "  - Data is extracted from a specified MongoDB collection and converted into a Pandas DataFrame. This conversion facilitates easy manipulation and analysis of the data.\n",
    "\n",
    "- **Data Processing**:\n",
    "  - The extracted DataFrame is processed through a predefined pipeline. This pipeline consists of a series of steps designed to transform and clean the data according to the specific requirements of the data model. Each step in the pipeline applies a particular operation, ensuring the data is prepared for its intended use.\n",
    "\n",
    "- **Data Insertion**:\n",
    "  - After processing, the transformed data is inserted into a different MongoDB collection. This step ensures that the processed data is stored efficiently, making it readily available for further analysis or application use.\n",
    "\n",
    "Overall, the `SecondStage` ensures that data is effectively managed, transformed, and stored, enhancing its utility and accessibility for subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "class MongoManagement:\n",
    "    \"\"\"\n",
    "    Class to manage MongoDB operations including exporting data to a DataFrame,\n",
    "    processing it through a pipeline, and inserting it into another collection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mongo_client: MongoClient, database: str):\n",
    "        \"\"\"\n",
    "        Initialize with a MongoDB client and database name.\n",
    "\n",
    "        :param mongo_client: The MongoDB client instance.\n",
    "        :param database: The name of the database.\n",
    "        \"\"\"\n",
    "        self.mongo_client = mongo_client\n",
    "        self.database = mongo_client[database]\n",
    "\n",
    "    def export_dataset(self, collection: str, path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Export dataset from MongoDB collection to a DataFrame and save as CSV.\n",
    "\n",
    "        :param collection: The name of the collection to export.\n",
    "        :param path: The file path to save the CSV.\n",
    "        :return: The exported DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            collection = self.database[collection]\n",
    "            df = pd.DataFrame(list(collection.find()))\n",
    "            df.to_csv(path, index=False)\n",
    "            print(\"Data exported successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return pd.DataFrame()  # Return an empty DataFrame in case of an error\n",
    "\n",
    "    def process_and_insert(self, df: pd.DataFrame, pipeline: 'Pipeline', target_collection: str):\n",
    "        \"\"\"\n",
    "        Process the DataFrame through a pipeline and insert the result into a target collection.\n",
    "\n",
    "        :param df: The DataFrame to process.\n",
    "        :param pipeline: The pipeline to process the data.\n",
    "        :param target_collection: The name of the target collection to insert data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_data = pipeline.execute(df)\n",
    "            self._prepare_and_insert(processed_data, target_collection)\n",
    "            print(\"Data processed and inserted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def add_df_to_collection(self, df: pd.DataFrame, collection: str):\n",
    "        \"\"\"\n",
    "        Add a DataFrame to a collection in MongoDB.\n",
    "\n",
    "        :param df: The DataFrame to add.\n",
    "        :param collection: The name of the collection to add the DataFrame to.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._prepare_and_insert(df, collection)\n",
    "            print(\"Data added to collection successfully.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def _prepare_and_insert(self, df: pd.DataFrame, collection: str):\n",
    "        \"\"\"\n",
    "        Prepare the DataFrame by handling NaT values and ensuring datetime columns are timezone-naive,\n",
    "        then insert it into the specified MongoDB collection.\n",
    "\n",
    "        :param df: The DataFrame to prepare and insert.\n",
    "        :param collection: The name of the collection to insert the DataFrame into.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle NaT values and ensure datetime columns are timezone-naive\n",
    "            datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "            for col in datetime_columns:\n",
    "                # Fill NaT with a default date\n",
    "                df[col] = df[col].fillna(pd.Timestamp('1970-01-01'))\n",
    "                # Ensure datetime columns are timezone-naive\n",
    "                df[col] = df[col].dt.tz_localize(None)\n",
    "\n",
    "            collection = self.database[collection]\n",
    "            collection.insert_many(df.to_dict('records'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting data into collection {collection}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- ThirdStage\n",
    "\n",
    "The `Identify Model` stage is focused on defining and understanding the data models involved in the data pipeline process. This stage is crucial for ensuring that data is accurately represented and processed according to its structure and intended use. It involves the following components:\n",
    "\n",
    "- **Input Model**:\n",
    "  - The input model represents the structure of the data as it is initially extracted from the source. It includes various fields that describe the attributes of the data, such as:\n",
    "    - `_id`: Unique identifier for each record.\n",
    "    - `creation_date`: The date the record was created.\n",
    "    - `purchase_date`: Optional date of purchase.\n",
    "    - `fiscal_year`: The fiscal year associated with the record.\n",
    "    - `lpa_number`, `purchase_order_number`, `requisition_number`: Various identifiers related to the acquisition process.\n",
    "    - `acquisition_type`, `sub_acquisition_type`, `acquisition_method`, `sub_acquisition_method`: Details about the acquisition process.\n",
    "    - `department_name`, `supplier_code`, `supplier_name`, `supplier_qualifications`, `supplier_zip_code`: Information about the department and supplier.\n",
    "    - `calcard`, `item_name`, `item_description`: Details about the items involved.\n",
    "    - `quantity`, `unit_price`, `total_price`: Financial details of the transaction.\n",
    "    - `classification_codes`, `normalized_unspsc`, `commodity_title`, `class_number`, `class_title`, `family`, `family_title`, `segment`, `segment_title`: Classification and categorization details.\n",
    "    - `location`: Optional location information.\n",
    "\n",
    "- **Output Model**:\n",
    "  - The output model will be identified and defined after applying the necessary processing steps in the pipeline. This model will reflect the transformed state of the data, tailored to meet the specific requirements of subsequent operations or analyses.\n",
    "\n",
    "This stage ensures that both the input and output data models are clearly defined, facilitating effective data processing and transformation throughout the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Input Model `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class InputModel:\n",
    "    _id: str\n",
    "    creation_date: str\n",
    "    purchase_date: Optional[str]\n",
    "    fiscal_year: str\n",
    "    lpa_number: Optional[str]\n",
    "    purchase_order_number: str\n",
    "    requisition_number: Optional[str]\n",
    "    acquisition_type: str\n",
    "    sub_acquisition_type: Optional[str]\n",
    "    acquisition_method: str\n",
    "    sub_acquisition_method: Optional[str]\n",
    "    department_name: str\n",
    "    supplier_code: Optional[float]\n",
    "    supplier_name: Optional[str]\n",
    "    supplier_qualifications: Optional[str]\n",
    "    supplier_zip_code: Optional[str]\n",
    "    calcard: str\n",
    "    item_name: Optional[str]\n",
    "    item_description: Optional[str]\n",
    "    quantity: Optional[float]\n",
    "    unit_price: Optional[str]\n",
    "    total_price: Optional[str]\n",
    "    classification_codes: Optional[str]\n",
    "    normalized_unspsc: Optional[float]\n",
    "    commodity_title: Optional[str]\n",
    "    class_number: Optional[float]\n",
    "    class_title: Optional[str]\n",
    "    family: Optional[float]\n",
    "    family_title: Optional[str]\n",
    "    segment: Optional[float]\n",
    "    segment_title: Optional[str]\n",
    "    location: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- FourthStage\n",
    "\n",
    "In the `Identifying Steps` stage, specific actions are determined based on data exploration and insights. This involves:\n",
    "\n",
    "- **Data Exploration**: Analyzing the data to understand its characteristics and identify areas needing transformation.\n",
    "- **Insight Gathering**: Documenting observations to inform decision-making.\n",
    "- **Decision Making**: Determining necessary transformations based on insights.\n",
    "- **Step Definition**: Defining specific steps for data cleaning, normalization, or feature extraction.\n",
    "- **Pipeline Integration**: Incorporating these steps into the pipeline in the correct sequence.\n",
    "\n",
    "This stage ensures the data pipeline is customized to improve data quality and usability for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step1` <br>\n",
    "This step defines a pipeline step to remove specific columns from a DataFrame, as more than **50% of those columns = `null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveSpecificColumnsStep(Step):\n",
    "    \"\"\"\n",
    "    Pipeline step to remove specific columns from the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # List of columns to remove\n",
    "        columns_to_remove = [\n",
    "            'LPA Number',\n",
    "            'Requisition Number',\n",
    "            'Sub-Acquisition Type',\n",
    "            'Sub-Acquisition Method',\n",
    "            'Supplier Qualifications'\n",
    "        ]\n",
    "        \n",
    "        # Drop the specified columns\n",
    "        data = data.drop(columns=columns_to_remove, errors='ignore')\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step2` <br>\n",
    "Will remove `Normalized UNSPSC` as it same value `Classification Codes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveNormalizedUNSPSCStep(Step):\n",
    "    \"\"\"\n",
    "    Removes the 'Normalized UNSPSC' column from the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Drop the 'Normalized UNSPSC' column, ignoring errors if it doesn't exist\n",
    "        data = data.drop(columns=['Normalized UNSPSC'], errors='ignore')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step3`<br>\n",
    "\n",
    "will remove the rows containg missing price values as those rows is samll values, and from `EDA` those meaningless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveMissingPriceRowsStep(Step):\n",
    "    \"\"\"\n",
    "    Removes rows with missing values in the 'Price' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Drop rows where 'Price' is missing\n",
    "        data = data.dropna(subset=['Unit Price'])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step4`<br>\n",
    "Perform One Hot Encoding to Faciel Year 2012-2013, 2013-2014, 2015 as 0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncodeFacielYearStep(Step):\n",
    "    \"\"\"\n",
    "    Performs one-hot encoding on the 'Faciel Year' column, mapping specific years to 0, 1, and 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Define the mapping for the 'Faciel Year' column\n",
    "        year_mapping = {\n",
    "            '2012-2013': 0,\n",
    "            '2013-2014': 1,\n",
    "            '2014-2015': 2\n",
    "        }\n",
    "        \n",
    "        # Map the 'Faciel Year' column using the defined mapping\n",
    "        data['Fiscal Year'] = data['Fiscal Year'].map(year_mapping)\n",
    "        \n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step5`<br>\n",
    "Transfer Creation Date and Purchase Date from str to datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToDateTimeStep(Step):\n",
    "    \"\"\"\n",
    "    Converts 'Creation Date' and 'Purchase Date' columns from strings to datetime, ignoring NaN values.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Convert 'Creation Date' to datetime, ignoring errors (e.g., NaN values)\n",
    "        data['Creation Date'] = pd.to_datetime(data['Creation Date'], errors='coerce')\n",
    "        \n",
    "        # Convert 'purchase_date' to datetime, ignoring errors (e.g., NaN values)\n",
    "        data['Purchase Date'] = pd.to_datetime(data['Purchase Date'], errors='coerce')\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step6`<br>\n",
    "Fill Missing Values in `Purchase Date` With The Mean Of each `Department Name`, take to fill the Purchase Date.<br>\n",
    "It is the difference in days between the Creation Date and the Purchase Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillMissingPurchaseDateStep(Step):\n",
    "    \"\"\"\n",
    "    Fills missing values in 'Purchase Date' with the mean difference in days\n",
    "    between 'Creation Date' and 'Purchase Date' for each 'Department Name'.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Ensure 'Creation Date' and 'Purchase Date' are datetime objects\n",
    "        data['Creation Date'] = pd.to_datetime(data['Creation Date'], format='%m/%d/%Y', errors='coerce')\n",
    "        data['Purchase Date'] = pd.to_datetime(data['Purchase Date'], errors='coerce')\n",
    "\n",
    "        # Calculate the difference in days between 'Creation Date' and 'Purchase Date'\n",
    "        data['days_diff'] = (data['Purchase Date'] - data['Creation Date']).dt.days\n",
    "\n",
    "        # Calculate the mean difference in days for each 'Department Name'\n",
    "        mean_days_diff = data.groupby('Department Name')['days_diff'].mean()\n",
    "\n",
    "        # Function to fill missing 'Purchase Date' values\n",
    "        def fill_purchase_date(row):\n",
    "            if pd.isna(row['Purchase Date']) and not pd.isna(row['Creation Date']):\n",
    "                mean_diff = mean_days_diff.get(row['Department Name'], 0)\n",
    "                # Ensure mean_diff is a valid number\n",
    "                if pd.notna(mean_diff):\n",
    "                    return row['Creation Date'] + pd.Timedelta(days=int(mean_diff))\n",
    "            return row['Purchase Date']\n",
    "\n",
    "        # Apply the function to fill missing 'Purchase Date' values\n",
    "        data['Purchase Date'] = data.apply(fill_purchase_date, axis=1)\n",
    "\n",
    "        # Drop the temporary 'days_diff' column\n",
    "        data = data.drop(columns=['days_diff'])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unit Test For Step6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def test_fill_missing_purchase_date():\n",
    "    # Create a sample DataFrame with 'Creation Date' in \"MM/DD/YYYY\" format\n",
    "    data = pd.DataFrame({\n",
    "        'Department Name': ['Dept A', 'Dept A', 'Dept B', 'Dept B', 'Dept C'],\n",
    "        'Creation Date': [\n",
    "            '01/01/2023', '01/05/2023', '02/01/2023', '02/10/2023', '03/01/2023'\n",
    "        ],\n",
    "        'Purchase Date': [\n",
    "            '01/03/2023', np.nan, '02/05/2023', np.nan, '03/05/2023'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Convert date columns to datetime with the specified format\n",
    "    data['Creation Date'] = pd.to_datetime(data['Creation Date'], format='%m/%d/%Y', errors='coerce')\n",
    "    data['Purchase Date'] = pd.to_datetime(data['Purchase Date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "    # Expected results\n",
    "    expected_purchase_dates = [\n",
    "        datetime(2023, 1, 3),  # No change\n",
    "        datetime(2023, 1, 7),  # Filled with mean difference for Dept A\n",
    "        datetime(2023, 2, 5),  # No change\n",
    "        datetime(2023, 2, 14), # Filled with mean difference for Dept B\n",
    "        datetime(2023, 3, 5)   # No change\n",
    "    ]\n",
    "\n",
    "    # Instantiate the step\n",
    "    step = FillMissingPurchaseDateStep()\n",
    "\n",
    "    # Apply the transformation\n",
    "    processed_data = step.run(data)\n",
    "\n",
    "    # Check the results\n",
    "    result_purchase_dates = processed_data['Purchase Date'].tolist()\n",
    "    assert result_purchase_dates == expected_purchase_dates, f\"Test failed! Expected {expected_purchase_dates} but got {result_purchase_dates}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_fill_missing_purchase_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step7`<br>\n",
    "\n",
    "Perform One-Hot-Encoding 0 for 'NO` and 1 for `YES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncodeYesNoStep(Step):\n",
    "    \"\"\"\n",
    "    Performs one-hot encoding on a specified column, mapping 'NO' to 0 and 'YES' to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column_name: str):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Map 'NO' to 0 and 'YES' to 1\n",
    "        data[self.column_name] = data[self.column_name].map({'NO': 0, 'YES': 1})\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step8`<br>\n",
    "Remove the column `Requisition Number` as it dublicate with `Purchase Order Number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveRequisitionNumberStep(Step):\n",
    "    \"\"\"\n",
    "    Removes the 'Requisition Number' column from the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Drop the 'Requisition Number' column, ignoring errors if it doesn't exist\n",
    "        data = data.drop(columns=['Requisition Number'], errors='ignore')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step9`<br>\n",
    "Remove the duplicated rows that is counted 2084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDuplicateRowsStep(Step):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows from the DataFrame, ignoring the '_Id' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Remove duplicates, ignoring the '_Id' column\n",
    "        data = data.loc[:, data.columns != '_id'].drop_duplicates().join(data['_id'])\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step10`<br>\n",
    "Split the Location string into 3 columns, `Zip Code`, `Latitude` and `Longitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitLocationStep(Step):\n",
    "    \"\"\"\n",
    "    Splits the 'Location' string into 'Zip Code', 'Latitude', and 'Longitude' columns,\n",
    "    and removes the original 'Location' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Initialize new columns with NaN\n",
    "        data['Zip Code'] = pd.NA\n",
    "        data['Latitude'] = pd.NA\n",
    "        data['Longitude'] = pd.NA\n",
    "\n",
    "        # Process only non-NaN 'Location' entries\n",
    "        non_nan_locations = data['Location'].dropna()\n",
    "\n",
    "        # Step 1: Split 'Location' into 'Zip Code' and 'Coordinates'\n",
    "        location_df = non_nan_locations.str.split('\\n', expand=True)\n",
    "        location_df.columns = ['Zip Code', 'Coordinates']\n",
    "\n",
    "        # Step 2: Split 'Coordinates' into 'Latitude' and 'Longitude'\n",
    "        coordinates_df = location_df['Coordinates'].str.split(',', expand=True)\n",
    "        coordinates_df.columns = ['Latitude', 'Longitude']\n",
    "        coordinates_df['Latitude'] = coordinates_df['Latitude'].str.replace('(', '', regex=False)\n",
    "        coordinates_df['Longitude'] = coordinates_df['Longitude'].str.replace(')', '', regex=False)\n",
    "\n",
    "        # Convert 'Latitude' and 'Longitude' to float with errors='coerce'\n",
    "        coordinates_df['Latitude'] = pd.to_numeric(coordinates_df['Latitude'], errors='coerce')\n",
    "        coordinates_df['Longitude'] = pd.to_numeric(coordinates_df['Longitude'], errors='coerce')\n",
    "\n",
    "        # Assign the extracted values back to the original DataFrame\n",
    "        data.loc[non_nan_locations.index, 'Zip Code'] = location_df['Zip Code']\n",
    "        data.loc[non_nan_locations.index, 'Latitude'] = coordinates_df['Latitude']\n",
    "        data.loc[non_nan_locations.index, 'Longitude'] = coordinates_df['Longitude']\n",
    "\n",
    "        # Remove the original 'Location' column\n",
    "        data = data.drop(columns=['Location'])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unit Test For Step10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "\n",
    "class TestSplitLocationStep(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Sample data for testing\n",
    "        self.data = pd.DataFrame({\n",
    "            'Location': [\n",
    "                '12345\\n(38.662263, -121.346136)',\n",
    "                '67890\\n(34.052235, -118.243683)',\n",
    "                None,\n",
    "                '54321\\n(40.712776, -74.005974)',\n",
    "                'InvalidFormat'\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Expected output\n",
    "        self.expected_data = pd.DataFrame({\n",
    "            'Zip Code': ['12345', '67890', None, '54321', None],\n",
    "            'Latitude': [38.662263, 34.052235, None, 40.712776, None],\n",
    "            'Longitude': [-121.346136, -118.243683, None, -74.005974, None]\n",
    "        })\n",
    "\n",
    "    def test_split_location(self):\n",
    "        # Instantiate the step\n",
    "        step = SplitLocationStep()\n",
    "\n",
    "        # Apply the transformation\n",
    "        result_data = step.run(self.data)\n",
    "\n",
    "        # Check the results\n",
    "        try:\n",
    "            assert_frame_equal(result_data, self.expected_data)\n",
    "            print(\"Test passed!\")\n",
    "        except AssertionError as e:\n",
    "            print(\"Test failed!\")\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- FifthStage\n",
    "Connect to MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the MongoDB URI from the environment variable\n",
    "# Get the MongoDB URI from the environment variable\n",
    "mongo_uri = os.getenv('MONGO_URI')\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(mongo_uri)\n",
    "\n",
    "# Initialize MongoManagement\n",
    "mongo_manager = MongoManagement(client, 'penny')\n",
    "\n",
    "# Export data from the 'penny' collection\n",
    "df = mongo_manager.export_dataset('penny', 'exported_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346018, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # Check the shape of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- SixStage\n",
    "\n",
    "This stage is performing all `PipelineSteps` to Exported DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:31:35,718 - INFO - Starting pipeline execution\n",
      "2024-11-09 17:31:35,720 - INFO - Executing step: RemoveSpecificColumnsStep\n",
      "2024-11-09 17:31:35,949 - INFO - Completed step: RemoveSpecificColumnsStep\n",
      "2024-11-09 17:31:35,950 - INFO - Executing step: RemoveNormalizedUNSPSCStep\n",
      "2024-11-09 17:31:36,225 - INFO - Completed step: RemoveNormalizedUNSPSCStep\n",
      "2024-11-09 17:31:36,226 - INFO - Executing step: RemoveMissingPriceRowsStep\n",
      "2024-11-09 17:31:36,579 - INFO - Completed step: RemoveMissingPriceRowsStep\n",
      "2024-11-09 17:31:36,581 - INFO - Executing step: OneHotEncodeFacielYearStep\n",
      "2024-11-09 17:31:36,630 - INFO - Completed step: OneHotEncodeFacielYearStep\n",
      "2024-11-09 17:31:36,631 - INFO - Executing step: ConvertToDateTimeStep\n",
      "2024-11-09 17:31:37,220 - INFO - Completed step: ConvertToDateTimeStep\n",
      "2024-11-09 17:31:37,221 - INFO - Executing step: FillMissingPurchaseDateStep\n",
      "2024-11-09 17:31:40,330 - INFO - Completed step: FillMissingPurchaseDateStep\n",
      "2024-11-09 17:31:40,331 - INFO - Executing step: OneHotEncodeYesNoStep\n",
      "2024-11-09 17:31:40,368 - INFO - Completed step: OneHotEncodeYesNoStep\n",
      "2024-11-09 17:31:40,369 - INFO - Executing step: RemoveRequisitionNumberStep\n",
      "2024-11-09 17:31:40,606 - INFO - Completed step: RemoveRequisitionNumberStep\n",
      "2024-11-09 17:31:40,606 - INFO - Executing step: RemoveDuplicateRowsStep\n",
      "2024-11-09 17:31:43,035 - INFO - Completed step: RemoveDuplicateRowsStep\n",
      "2024-11-09 17:31:43,036 - INFO - Executing step: SplitLocationStep\n",
      "2024-11-09 17:31:44,313 - INFO - Completed step: SplitLocationStep\n",
      "2024-11-09 17:31:44,315 - INFO - Pipeline execution completed\n"
     ]
    }
   ],
   "source": [
    "# Identify the steps to be executed\n",
    "steps = [\n",
    "    RemoveSpecificColumnsStep(),\n",
    "    RemoveNormalizedUNSPSCStep(),\n",
    "    RemoveMissingPriceRowsStep(),\n",
    "    OneHotEncodeFacielYearStep(),\n",
    "    ConvertToDateTimeStep(),\n",
    "    FillMissingPurchaseDateStep(),\n",
    "    OneHotEncodeYesNoStep('CalCard'),\n",
    "    RemoveRequisitionNumberStep(),\n",
    "    RemoveDuplicateRowsStep(),\n",
    "    SplitLocationStep()\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Apply the pipeline\n",
    "cleaned_df = pipeline.execute(df)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343904, 28)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.shape # Check the shape of the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- SevenStage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:48:43,351 - INFO - Starting pipeline execution\n",
      "2024-11-09 17:48:43,352 - INFO - Executing step: RemoveSpecificColumnsStep\n",
      "2024-11-09 17:48:43,709 - INFO - Completed step: RemoveSpecificColumnsStep\n",
      "2024-11-09 17:48:43,709 - INFO - Executing step: RemoveNormalizedUNSPSCStep\n",
      "2024-11-09 17:48:43,992 - INFO - Completed step: RemoveNormalizedUNSPSCStep\n",
      "2024-11-09 17:48:43,993 - INFO - Executing step: RemoveMissingPriceRowsStep\n",
      "2024-11-09 17:48:44,350 - INFO - Completed step: RemoveMissingPriceRowsStep\n",
      "2024-11-09 17:48:44,350 - INFO - Executing step: OneHotEncodeFacielYearStep\n",
      "2024-11-09 17:48:44,400 - INFO - Completed step: OneHotEncodeFacielYearStep\n",
      "2024-11-09 17:48:44,400 - INFO - Executing step: ConvertToDateTimeStep\n",
      "2024-11-09 17:48:44,955 - INFO - Completed step: ConvertToDateTimeStep\n",
      "2024-11-09 17:48:44,956 - INFO - Executing step: FillMissingPurchaseDateStep\n",
      "2024-11-09 17:48:48,466 - INFO - Completed step: FillMissingPurchaseDateStep\n",
      "2024-11-09 17:48:48,466 - INFO - Executing step: OneHotEncodeYesNoStep\n",
      "2024-11-09 17:48:48,498 - INFO - Completed step: OneHotEncodeYesNoStep\n",
      "2024-11-09 17:48:48,499 - INFO - Executing step: RemoveRequisitionNumberStep\n",
      "2024-11-09 17:48:48,715 - INFO - Completed step: RemoveRequisitionNumberStep\n",
      "2024-11-09 17:48:48,716 - INFO - Executing step: RemoveDuplicateRowsStep\n",
      "2024-11-09 17:48:50,857 - INFO - Completed step: RemoveDuplicateRowsStep\n",
      "2024-11-09 17:48:50,858 - INFO - Executing step: SplitLocationStep\n",
      "2024-11-09 17:48:52,403 - INFO - Completed step: SplitLocationStep\n",
      "2024-11-09 17:48:52,404 - INFO - Pipeline execution completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed and inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "''' Export the cleaned DataFrame to a MonoDB new collection'''\n",
    "mongo_manager.process_and_insert(df, pipeline, 'Pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8- Eight Step \n",
    "\n",
    "Indetify `OutPutModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Creation Date            datetime64[ns]\n",
       "Purchase Date            datetime64[ns]\n",
       "Fiscal Year                       int64\n",
       "Purchase Order Number            object\n",
       "Acquisition Type                 object\n",
       "Acquisition Method               object\n",
       "Department Name                  object\n",
       "Supplier Code                   float64\n",
       "Supplier Name                    object\n",
       "Supplier Zip Code                object\n",
       "CalCard                           int64\n",
       "Item Name                        object\n",
       "Item Description                 object\n",
       "Quantity                        float64\n",
       "Unit Price                       object\n",
       "Total Price                      object\n",
       "Classification Codes             object\n",
       "Commodity Title                  object\n",
       "Class                           float64\n",
       "Class Title                      object\n",
       "Family                          float64\n",
       "Family Title                     object\n",
       "Segment                         float64\n",
       "Segment Title                    object\n",
       "_id                              object\n",
       "Zip Code                         object\n",
       "Latitude                         object\n",
       "Longitude                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.dtypes # Check the data types of the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import datetime\n",
    "\n",
    "@dataclass\n",
    "class OutputModel:\n",
    "    creation_date: Optional[datetime.datetime]\n",
    "    purchase_date: Optional[datetime.datetime]\n",
    "    fiscal_year: Optional[int]\n",
    "    purchase_order_number: Optional[str]\n",
    "    acquisition_type: Optional[str]\n",
    "    acquisition_method: Optional[str]\n",
    "    department_name: Optional[str]\n",
    "    supplier_code: Optional[float]\n",
    "    supplier_name: Optional[str]\n",
    "    supplier_zip_code: Optional[str]\n",
    "    calcard: Optional[int]\n",
    "    item_name: Optional[str]\n",
    "    item_description: Optional[str]\n",
    "    quantity: Optional[float]\n",
    "    unit_price: Optional[str]\n",
    "    total_price: Optional[str]\n",
    "    classification_codes: Optional[str]\n",
    "    commodity_title: Optional[str]\n",
    "    class_: Optional[float]  # 'class' is a reserved keyword in Python, so use 'class_'\n",
    "    class_title: Optional[str]\n",
    "    family: Optional[float]\n",
    "    family_title: Optional[str]\n",
    "    segment: Optional[float]\n",
    "    segment_title: Optional[str]\n",
    "    _id: Optional[str]\n",
    "    zip_code: Optional[str]\n",
    "    latitude: Optional[str]\n",
    "    longitude: Optional[str]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
